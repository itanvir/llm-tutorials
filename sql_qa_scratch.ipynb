{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2e0bab-fae5-47bb-9138-5f2306a8d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5506e09c-78d1-4ac2-b3e0-557e8e76c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset\n",
    "dataset = load_dataset('b-mc2/sql-create-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97c2e136-3732-4b8a-9687-417d695849f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [example['context'] for example in dataset['train']]\n",
    "answers = [example['answer'] for example in dataset['train']]\n",
    "questions = [example['question'] for example in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b765afe-7f89-4e25-a3e5-0010b211fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming contexts, answers, and questions are your dataset\n",
    "data = list(zip(contexts, answers, questions))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Unzip the data\n",
    "train_contexts, train_answers, train_questions = map(list, zip(*train_data))\n",
    "val_contexts, val_answers, val_questions = map(list, zip(*val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7809b4-07e4-4fac-9d93-be9d1b1eed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepare_text(text):\n",
    "    text = '<START>' + text + '<END>'\n",
    "    return text\n",
    "\n",
    "train_answers = list(map(prepare_text, train_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a4b82b-4998-4e06-8722-f9bebb9a3975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, trainers, models, pre_tokenizers, decoders, processors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Set padding token. Padding token must be zero.\n",
    "tokenizer.add_special_tokens([\"<PAD>\", \"<START>\", \"<END>\"])\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"<PAD>\")\n",
    "\n",
    "# Gather all texts\n",
    "all_texts = train_contexts + train_questions + train_answers\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(all_texts)\n",
    "\n",
    "# Now you can use the tokenizer to encode your texts\n",
    "prompt_tokens, answer_tokens = [], []\n",
    "for context, question, answer in zip(train_contexts, train_questions, train_answers):\n",
    "    prompt = question + \" context: \" + context\n",
    "    prompt_tokens.append(tokenizer.encode(prompt).ids)\n",
    "    answer_tokens.append(tokenizer.encode(answer).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d759f44-1557-4e6f-a070-adeff5a345e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<END>\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6440a4cd-cf72-4fa7-a921-a7bb680aed47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.9433e+04, 1.9802e+04, 3.1700e+03, 3.7200e+02, 6.8000e+01,\n",
       "        1.1000e+01, 0.0000e+00, 3.0000e+00, 0.0000e+00, 2.0000e+00]),\n",
       " array([  7. ,  19.4,  31.8,  44.2,  56.6,  69. ,  81.4,  93.8, 106.2,\n",
       "        118.6, 131. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_lengths = [len(seq) for seq in prompt_tokens]\n",
    "plt.hist(seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bbf7d4f-a298-4dba-b2b4-d3609c89e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_seq_length = 50  # replace with your desired sequence length\n",
    "answer_seq_length = 50\n",
    "\n",
    "# Add padding if necessary\n",
    "prompt_tokens_padded = tf.keras.preprocessing.sequence.pad_sequences(prompt_tokens, maxlen=prompt_seq_length, padding='post', value=0)\n",
    "answer_tokens_padded = tf.keras.preprocessing.sequence.pad_sequences(answer_tokens, maxlen=answer_seq_length, padding='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac50b3-959e-41de-aca2-91c20b78ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29173a2f-6fcf-4381-b62b-cd879250a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "\n",
    "@tf.keras.saving.register_keras_serializable()\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, feedforward_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(feedforward_dim, activation=\"relu\"), tf.keras.layers.Dense(embedding_dim),]\n",
    "        )\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "\n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(dff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "@tf.keras.saving.register_keras_serializable()\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads, feedforward_dim):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "\n",
    "        self.multi_head_attention1 = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim)\n",
    "        self.multi_head_attention2 = tf.keras.layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim)\n",
    "        self.feed_forward = FeedForward(self.embedding_dim, self.feedforward_dim)\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def create_lookahead_mask(self, tensor):\n",
    "        # Get the sequence length from the tensor shape\n",
    "        seq_length = tf.shape(tensor)[1]\n",
    "    \n",
    "        # Create a lookahead mask\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
    "    \n",
    "        # Expand dimensions to match batch size\n",
    "        mask = tf.expand_dims(mask, 0)  \n",
    "    \n",
    "        # Tile mask to match batch size\n",
    "        mask = tf.tile(mask, [tf.shape(tensor)[0], 1, 1])\n",
    "    \n",
    "        # Convert to boolean\n",
    "        mask = tf.cast(mask, dtype=tf.bool)\n",
    "    \n",
    "        return mask\n",
    "\n",
    "    def call(self, decoder_embeddings, encoder_output):\n",
    "        #lookahead_mask = self.create_lookahead_mask(decoder_embeddings)\n",
    "        attention_output1 = self.multi_head_attention1(decoder_embeddings, decoder_embeddings, decoder_embeddings, use_causal_mask = True)\n",
    "        attention_output1 = self.layernorm1(attention_output1 + decoder_embeddings)\n",
    "        attention_output2 = self.multi_head_attention2(attention_output1, encoder_output, encoder_output)\n",
    "        attention_output2 = self.layernorm2(attention_output2 + attention_output1)\n",
    "        feed_forward_output = self.feed_forward(attention_output2)\n",
    "        decoder_output = self.layernorm3(feed_forward_output + attention_output2)\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417d49e6-3e79-4ed0-8f6a-c02aadeaa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.saving.register_keras_serializable()\n",
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a71466a-e060-4fa1-9414-0b290ddf8ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 11:32:14.934553: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-05-24 11:32:14.934571: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-05-24 11:32:14.934575: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-05-24 11:32:14.934600: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-24 11:32:14.934612: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                        Param #     Connected to                        \n",
      "========================================================================================================================\n",
      " encoder_input (InputLayer)         [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " decoder_input (InputLayer)         [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " embedding (Embedding)              (None, None, 64)                    1920128     ['encoder_input[0][0]']             \n",
      "                                                                                                                        \n",
      " embedding_1 (Embedding)            (None, None, 64)                    1920128     ['decoder_input[0][0]']             \n",
      "                                                                                                                        \n",
      " position_embedding (PositionEmbed  (None, None, 64)                    3200        ['embedding[0][0]']                 \n",
      " ding)                                                                                                                  \n",
      "                                                                                                                        \n",
      " position_embedding_1 (PositionEmb  (None, None, 64)                    3200        ['embedding_1[0][0]']               \n",
      " edding)                                                                                                                \n",
      "                                                                                                                        \n",
      " add (Add)                          (None, None, 64)                    0           ['embedding[0][0]',                 \n",
      "                                                                                     'position_embedding[0][0]']        \n",
      "                                                                                                                        \n",
      " add_1 (Add)                        (None, None, 64)                    0           ['embedding_1[0][0]',               \n",
      "                                                                                     'position_embedding_1[0][0]']      \n",
      "                                                                                                                        \n",
      " transformer_encoder (TransformerE  (None, None, 64)                    199040      ['add[0][0]']                       \n",
      " ncoder)                                                                                                                \n",
      "                                                                                                                        \n",
      " transformer_decoder (TransformerD  (None, None, 64)                    331840      ['add_1[0][0]',                     \n",
      " ecoder)                                                                             'transformer_encoder[0][0]']       \n",
      "                                                                                                                        \n",
      " dropout_2 (Dropout)                (None, None, 64)                    0           ['transformer_decoder[0][0]']       \n",
      "                                                                                                                        \n",
      " dense_4 (Dense)                    (None, None, 30002)                 1950130     ['dropout_2[0][0]']                 \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 6327666 (24.14 MB)\n",
      "Trainable params: 6327666 (24.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding\n",
    "from keras_nlp.layers import ReversibleEmbedding, PositionEmbedding\n",
    "#from keras_nlp.layers import TransformerDecoder\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "sequence_length = 50\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "num_heads = 8\n",
    "embedding_dim = 64\n",
    "feedforward_dim = 512\n",
    "\n",
    "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
    "encoder_token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(encoder_input)\n",
    "encoder_position_embeddings = PositionEmbedding(sequence_length=sequence_length)(encoder_token_embeddings)\n",
    "encoder_embeddings = tf.keras.layers.Add()([encoder_token_embeddings, encoder_position_embeddings])\n",
    "encoder_output = TransformerEncoder(embedding_dim, num_heads, feedforward_dim)(encoder_embeddings)\n",
    "\n",
    "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
    "decoder_token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(decoder_input)\n",
    "decoder_position_embeddings = PositionEmbedding(sequence_length=sequence_length)(decoder_token_embeddings)\n",
    "decoder_embeddings = tf.keras.layers.Add()([decoder_token_embeddings, decoder_position_embeddings])\n",
    "decoder_output = TransformerDecoder(embedding_dim, num_heads, feedforward_dim)(decoder_embeddings, encoder_output)\n",
    "#decoder_output = TransformerDecoder(feedforward_dim, num_heads)(decoder_embeddings, encoder_output)\n",
    "decoder_output = Dropout(0.4)(decoder_output)\n",
    "\n",
    "final_output = Dense(vocab_size, activation='softmax')(decoder_output)\n",
    "model = Model([encoder_input, decoder_input], final_output)\n",
    "model.compile(optimizer='adam', loss=masked_loss, metrics=['accuracy'])\n",
    "model.summary(line_length=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b1cdf2-88db-43a6-81d1-3263ed44dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = { 'encoder_input': prompt_tokens_padded, 'decoder_input': answer_tokens_padded[:, :-1] }\n",
    "outputs = answer_tokens_padded[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bdb628f-235e-476d-a574-eb82135f4354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 11:32:22.699057: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1572/1572 [==============================] - ETA: 0s - loss: 5.9098 - accuracy: 0.0711\n",
      "Epoch 1: val_accuracy improved from -inf to 0.08014, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 171s 108ms/step - loss: 5.9098 - accuracy: 0.0711 - val_loss: 4.9503 - val_accuracy: 0.0801\n",
      "Epoch 2/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 4.5741 - accuracy: 0.0873\n",
      "Epoch 2: val_accuracy improved from 0.08014 to 0.09759, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 167s 106ms/step - loss: 4.5741 - accuracy: 0.0873 - val_loss: 4.1215 - val_accuracy: 0.0976\n",
      "Epoch 3/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 3.8037 - accuracy: 0.1063\n",
      "Epoch 3: val_accuracy improved from 0.09759 to 0.12155, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 170s 108ms/step - loss: 3.8037 - accuracy: 0.1063 - val_loss: 3.4456 - val_accuracy: 0.1215\n",
      "Epoch 4/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 3.0294 - accuracy: 0.1345\n",
      "Epoch 4: val_accuracy improved from 0.12155 to 0.15093, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 170s 108ms/step - loss: 3.0294 - accuracy: 0.1345 - val_loss: 2.7715 - val_accuracy: 0.1509\n",
      "Epoch 5/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 2.3948 - accuracy: 0.1583\n",
      "Epoch 5: val_accuracy improved from 0.15093 to 0.16648, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 165s 105ms/step - loss: 2.3948 - accuracy: 0.1583 - val_loss: 2.3798 - val_accuracy: 0.1665\n",
      "Epoch 6/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 1.9356 - accuracy: 0.1751\n",
      "Epoch 6: val_accuracy improved from 0.16648 to 0.17736, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 166s 106ms/step - loss: 1.9356 - accuracy: 0.1751 - val_loss: 2.1223 - val_accuracy: 0.1774\n",
      "Epoch 7/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 1.5885 - accuracy: 0.1887\n",
      "Epoch 7: val_accuracy improved from 0.17736 to 0.18519, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 162s 103ms/step - loss: 1.5885 - accuracy: 0.1887 - val_loss: 1.9356 - val_accuracy: 0.1852\n",
      "Epoch 8/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 1.3369 - accuracy: 0.1988\n",
      "Epoch 8: val_accuracy improved from 0.18519 to 0.18930, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 163s 104ms/step - loss: 1.3369 - accuracy: 0.1988 - val_loss: 1.8651 - val_accuracy: 0.1893\n",
      "Epoch 9/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 1.1521 - accuracy: 0.2064\n",
      "Epoch 9: val_accuracy improved from 0.18930 to 0.19338, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 166s 106ms/step - loss: 1.1521 - accuracy: 0.2064 - val_loss: 1.8001 - val_accuracy: 0.1934\n",
      "Epoch 10/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 1.0097 - accuracy: 0.2127\n",
      "Epoch 10: val_accuracy improved from 0.19338 to 0.19602, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 162s 103ms/step - loss: 1.0097 - accuracy: 0.2127 - val_loss: 1.7811 - val_accuracy: 0.1960\n",
      "Epoch 11/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.9009 - accuracy: 0.2174\n",
      "Epoch 11: val_accuracy improved from 0.19602 to 0.19853, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 161s 103ms/step - loss: 0.9009 - accuracy: 0.2174 - val_loss: 1.7811 - val_accuracy: 0.1985\n",
      "Epoch 12/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.8145 - accuracy: 0.2213\n",
      "Epoch 12: val_accuracy improved from 0.19853 to 0.20002, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 165s 105ms/step - loss: 0.8145 - accuracy: 0.2213 - val_loss: 1.7711 - val_accuracy: 0.2000\n",
      "Epoch 13/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.7445 - accuracy: 0.2247\n",
      "Epoch 13: val_accuracy improved from 0.20002 to 0.20138, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 165s 105ms/step - loss: 0.7445 - accuracy: 0.2247 - val_loss: 1.7969 - val_accuracy: 0.2014\n",
      "Epoch 14/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.2275\n",
      "Epoch 14: val_accuracy improved from 0.20138 to 0.20222, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 167s 106ms/step - loss: 0.6841 - accuracy: 0.2275 - val_loss: 1.7865 - val_accuracy: 0.2022\n",
      "Epoch 15/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.2299\n",
      "Epoch 15: val_accuracy improved from 0.20222 to 0.20248, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 167s 106ms/step - loss: 0.6354 - accuracy: 0.2299 - val_loss: 1.8260 - val_accuracy: 0.2025\n",
      "Epoch 16/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.2322\n",
      "Epoch 16: val_accuracy improved from 0.20248 to 0.20343, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 168s 107ms/step - loss: 0.5911 - accuracy: 0.2322 - val_loss: 1.8016 - val_accuracy: 0.2034\n",
      "Epoch 17/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.2340\n",
      "Epoch 17: val_accuracy improved from 0.20343 to 0.20461, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 167s 106ms/step - loss: 0.5541 - accuracy: 0.2340 - val_loss: 1.8235 - val_accuracy: 0.2046\n",
      "Epoch 18/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.2360\n",
      "Epoch 18: val_accuracy improved from 0.20461 to 0.20471, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 169s 108ms/step - loss: 0.5212 - accuracy: 0.2360 - val_loss: 1.8335 - val_accuracy: 0.2047\n",
      "Epoch 19/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.4917 - accuracy: 0.2375\n",
      "Epoch 19: val_accuracy improved from 0.20471 to 0.20488, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 163s 104ms/step - loss: 0.4917 - accuracy: 0.2375 - val_loss: 1.8711 - val_accuracy: 0.2049\n",
      "Epoch 20/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.4653 - accuracy: 0.2390\n",
      "Epoch 20: val_accuracy improved from 0.20488 to 0.20598, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 170s 108ms/step - loss: 0.4653 - accuracy: 0.2390 - val_loss: 1.8827 - val_accuracy: 0.2060\n",
      "Epoch 21/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.2402\n",
      "Epoch 21: val_accuracy improved from 0.20598 to 0.20599, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 172s 109ms/step - loss: 0.4414 - accuracy: 0.2402 - val_loss: 1.9039 - val_accuracy: 0.2060\n",
      "Epoch 22/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.2412\n",
      "Epoch 22: val_accuracy improved from 0.20599 to 0.20631, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 169s 108ms/step - loss: 0.4243 - accuracy: 0.2412 - val_loss: 1.8874 - val_accuracy: 0.2063\n",
      "Epoch 23/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.4030 - accuracy: 0.2425\n",
      "Epoch 23: val_accuracy improved from 0.20631 to 0.20713, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 165s 105ms/step - loss: 0.4030 - accuracy: 0.2425 - val_loss: 1.9528 - val_accuracy: 0.2071\n",
      "Epoch 24/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.3852 - accuracy: 0.2435\n",
      "Epoch 24: val_accuracy improved from 0.20713 to 0.20737, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 171s 109ms/step - loss: 0.3852 - accuracy: 0.2435 - val_loss: 1.9278 - val_accuracy: 0.2074\n",
      "Epoch 25/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.2444\n",
      "Epoch 25: val_accuracy improved from 0.20737 to 0.20783, saving model to model/sql_qa_model.keras\n",
      "1572/1572 [==============================] - 169s 108ms/step - loss: 0.3687 - accuracy: 0.2444 - val_loss: 1.9924 - val_accuracy: 0.2078\n",
      "Epoch 26/40\n",
      "1572/1572 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.2452\n",
      "Epoch 26: val_accuracy did not improve from 0.20783\n",
      "1572/1572 [==============================] - 165s 105ms/step - loss: 0.3563 - accuracy: 0.2452 - val_loss: 1.9832 - val_accuracy: 0.2075\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "checkpoint_filepath = 'model/sql_qa_model.keras'\n",
    "callback_es = EarlyStopping(monitor='val_accuracy', restore_best_weights=True)\n",
    "callback_mc = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "history = model.fit(inputs, outputs, epochs=40, validation_split=0.2, callbacks=[callback_es, callback_mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "177e918a-8598-4831-b438-7c0fdb6b3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"model/sql_qa_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "525ba61c-0b2c-4b69-ae39-968fcb9a1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bot_answer(text, model, tokenizer, sequence_length):\n",
    "\n",
    "    text_encoded = tokenizer.encode(text)\n",
    "    encoder_input = text_encoded.ids\n",
    "    encoder_input = tf.keras.preprocessing.sequence.pad_sequences([encoder_input], maxlen=sequence_length, padding='post', value=0)\n",
    "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "    encoder_input = tf.reshape(encoder_input, (1, 50))\n",
    "    \n",
    "    decoded_text = '<START>'\n",
    "    for i in range(sequence_length):\n",
    "        decoder_input = tokenizer.encode(decoded_text).ids\n",
    "        decoder_input = tf.keras.preprocessing.sequence.pad_sequences([decoder_input], maxlen=sequence_length, padding='post', value=0)\n",
    "        decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "        decoder_input = tf.reshape(decoder_input, (1, 50))\n",
    "    \n",
    "        prediction = model([encoder_input, decoder_input])\n",
    "    \n",
    "        idx = np.argmax(prediction[0, i, :])\n",
    "        token = tokenizer.decode([idx])\n",
    "        decoded_text += token\n",
    "\n",
    "        if token == '<END>':\n",
    "            break\n",
    "\n",
    "    return decoded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c196d09-6942-4981-b530-d71711b49e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62861, 50)\n",
      "(62861, 49)\n",
      "(62861, 49)\n"
     ]
    }
   ],
   "source": [
    "print (inputs['encoder_input'].shape)\n",
    "print (inputs['decoder_input'].shape)\n",
    "print (outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "850cc2dd-014a-4b7f-ab21-d4d57e9f6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = model([inputs['encoder_input'][0:10], inputs['decoder_input'][0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd1fe69e-9439-4af8-9176-e89b19b5ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: What was the number of Laps with a Grid of more than 3 and Time of 39:24.967? context: CREATE TABLE table_name_8 (laps INTEGER, grid VARCHAR, time VARCHAR)\n",
      "BOT: <START>SELECT\"\"\"\" AND  = \"\"\"\"\"\"\"\"\"\"\"\"ooo\" = \" FROM table_name_2 WHERE oso_s_oshomosioio4 WHERE  FROM table_name_8 WHERE oolaps) FROM table_name_o\"oround FROM table_name_points) FROM table_name_\"\" AND points > \": AND team = \"\"league) FROM table_name_\n",
      "GroundTruth: SELECT MIN(laps) FROM table_name_8 WHERE grid > 3 AND time = \"39:24.967\"\n",
      "-------------\n",
      "PROMPT: Which Senior status has a Chief Judge of , a Reason for termination of death, and Active service of 19671983? context: CREATE TABLE table_name_18 (senior_status VARCHAR, active_service VARCHAR, chief_judge VARCHAR, reason_for_termination VARCHAR)\n",
      "BOT: <START>SELECT senior_status FROM table_name_18 WHERE active_known_for_termination = \"\" AND reason_for_termination = \"yes\" AND body93633\"\"\"\"\"\"\"\"\"\"\"\"\"ananoentdat\"anent\"ml\n",
      "GroundTruth: SELECT senior_status FROM table_name_18 WHERE chief_judge = \"\" AND reason_for_termination = \"death\" AND active_service = \"19671983\"\n",
      "-------------\n",
      "PROMPT: Which Rank has a Reaction of 0.198, and a Time smaller than 46.3? context: CREATE TABLE table_name_98 (rank INTEGER, react VARCHAR, time VARCHAR)\n",
      "BOT: <START>SELECT MAX(rank) FROM table_name_98 WHERE react = 0.198 AND time < 0.\"\"\"\"\"\"\"\"\"\"\"\"placioosioter FROM table_name_9 WHERE time = \"0.98th\"terreactioter\"SUM(rank) FROM table_name_93\"action1\"action\n",
      "GroundTruth: SELECT MAX(rank) FROM table_name_98 WHERE react = 0.198 AND time < 46.3\n",
      "-------------\n",
      "PROMPT: Who narrated when the vessel operator is de beers? context: CREATE TABLE table_26168687_3 (narrated_by VARCHAR, vessel_operator VARCHAR)\n",
      "BOT: <START>SELECT\"\"_____e\"\"\"\"\"\"\"\"\"\"ondistrict FROM table_18onon) FROM table_26 FROM table_18 FROM table_18ran FROM table_name_50 WHERE ) FROM table_134ran FROM table_name_50 WHERE semifinalistranranrrike \" FROM table_28_36 WHERE 7_3 WHERE rr7_3 WHERE \"ly\n",
      "GroundTruth: SELECT narrated_by FROM table_26168687_3 WHERE vessel_operator = \"De Beers\"\n",
      "-------------\n",
      "PROMPT: What's the original season in 11th place? context: CREATE TABLE table_name_15 (original_season VARCHAR, placing VARCHAR)\n",
      "BOT: <START>SELECT\"\"\" = \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" = \"seasononononrgroup_group_group_ FROM table_name_27 WHERE ongroup_seasonrunner_up\" = \"seasonv\" FROM table_name_15 WHERE \" = \"season\"\" = \"\"4) FROM table_name_10 WHERE \"season\n",
      "GroundTruth: SELECT original_season FROM table_name_15 WHERE placing = \"11th place\"\n",
      "-------------\n",
      "PROMPT: What is the Place of the Song by Artist Rosie Hunter with a Draw of 1 or larger? context: CREATE TABLE table_name_10 (place VARCHAR, artist VARCHAR, draw VARCHAR)\n",
      "BOT: <START>SELECT\"\"\"\" = \"\"\"\" AND \"\"\"\"\"\"\"\"\"\"o\" = \"placplacoplacplaceventranler\"visitorplacplacoconstituency_number = \"ler\"entvideo\"rsr) FROM table_name_10 WHERE \"ter\n",
      "GroundTruth: SELECT COUNT(place) FROM table_name_10 WHERE artist = \"rosie hunter\" AND draw > 1\n",
      "-------------\n",
      "PROMPT: Show the times of elimination by \"Punk\" or \"Orton\". context: CREATE TABLE elimination (TIME VARCHAR, Eliminated_By VARCHAR)\n",
      "BOT: <START>SELECT TIME FROM elimination (elimination GROUP BY Eliminated_list WHERE Eliminated_y = \"ORDER BY Eliminated_hortonton\"\"\"\")\"\"\" (\"t)\"\"\"it\" AS\"ORDER BY COUNT(*) DESC LIMIT ton\", t1.\n",
      "GroundTruth: SELECT TIME FROM elimination WHERE Eliminated_By = \"Punk\" OR Eliminated_By = \"Orton\"\n",
      "-------------\n",
      "PROMPT: Where did they play the San Diego Chargers? context: CREATE TABLE table_13259009_2 (game_site VARCHAR, opponent VARCHAR)\n",
      "BOT: <START>SELECT\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"rgroup_rgroup_group_resultopponent) FROM table_13) FROM table_134incumbent) FROM table_134result27700375opponent = \"result = \"wweekopponentopponent = \"ropponent\" AND opponent = \" tvresult = \"w\"opponent = \"opponentgame_sit277ado\"\"championships\" AND \n",
      "GroundTruth: SELECT game_site FROM table_13259009_2 WHERE opponent = \"San Diego Chargers\"\n",
      "-------------\n",
      "PROMPT: Which Played has a Lost larger than 2, and a Team of amrica? context: CREATE TABLE table_name_4 (played VARCHAR, lost VARCHAR, team VARCHAR)\n",
      "BOT: <START>SELECT\"\"\"\"\"aaa\"\"\"\"\"\"\"\"\"\"lost FROM table_name_2 WHERE tonsioslost\" AND points > irs7 WHERE  FROM table_name_2 WHERE so\" AND points > lost\" AND team = \"\"\"o22\"round FROM table_name_\n",
      "GroundTruth: SELECT played FROM table_name_4 WHERE lost > 2 AND team = \"amrica\"\n",
      "-------------\n",
      "PROMPT: What venue has a score of 4-0 with the 2002 Tiger Cup listed as the competition? context: CREATE TABLE table_name_26 (venue VARCHAR, score VARCHAR, competition VARCHAR)\n",
      "BOT: <START>SELECT\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" FROM table_name_26 WHERE annsemifinalist FROM table_name_26 WHERE ioton = \"3-04-0\" FROM table_name_26 WHERE venu FROM table_name_26 WHERE semifinalisthomvenuan\"silverround) FROM table_name_score_in_final\" = \"3r\"\"4-0) FROM table_name_26 WHERE venucong\n",
      "GroundTruth: SELECT venue FROM table_name_26 WHERE score = \"4-0\" AND competition = \"2002 tiger cup\"\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    question = val_questions[i]\n",
    "    context = val_contexts[i]\n",
    "    answer = val_answers[i]\n",
    "    prompt = question + \" context: \" + context\n",
    "    print (\"PROMPT:\", prompt)\n",
    "    decoded_text = get_bot_answer(prompt, model, tokenizer, sequence_length)\n",
    "    print (\"BOT:\", decoded_text)\n",
    "    print (\"GroundTruth:\", answer)\n",
    "    print (\"-------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7422f-63de-4bad-937d-6aba406849ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
