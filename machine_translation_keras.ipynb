{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76ee634-a92e-4ef4-8fe3-f006f42f728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55010993-1a92-4c5c-9f56-3ae79494b952",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a41e9d8-703d-4c9b-afde-28ae6412f1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 12:48:57.684693: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Max\n",
      "2024-05-22 12:48:57.684710: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2024-05-22 12:48:57.684716: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2024-05-22 12:48:57.684742: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-05-22 12:48:57.684755: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                               with_info=True,\n",
    "                               as_supervised=True)\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1db7fa-fbca-405b-b046-da68b6cc13a0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69738f66-f409-4cac-b127-90ac5a905026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the tokenizer on your text\n",
    "pt_sentences = []\n",
    "en_sentences = []\n",
    "\n",
    "for pt, en in train_examples:\n",
    "    pt_sentences.append(pt.numpy().decode('utf-8'))\n",
    "    en_sentences.append(en.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e654547-3497-4101-b822-f411fe528dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "def clean_text(text):\n",
    "    text = normalize('NFD', text.lower())\n",
    "    text = re.sub('[^A-Za-z ]+', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_and_prepare_text(text):\n",
    "    text = '[start] ' + clean_text(text) + ' [end]'\n",
    "    return text\n",
    "\n",
    "pt_sentences = list(map(clean_text, pt_sentences))\n",
    "en_sentences = list(map(clean_and_prepare_text, en_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da28e0b8-9d55-43d4-b6e6-932ff0afda26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Vocabulary Size:  30000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "\n",
    "sequence_len = 50\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Customize pre-tokenization and decoding\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Enable padding\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\", length=sequence_len)\n",
    "\n",
    "# Enable truncation\n",
    "tokenizer.enable_truncation(max_length=sequence_len)\n",
    "\n",
    "# Fit the tokenizer on your text\n",
    "tokenizer.train_from_iterator(pt_sentences + en_sentences)\n",
    "\n",
    "# Get vocab size\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print('Vocabulary Size: ', vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49984ac9-9b08-4ae8-8022-549a43567e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokens = []\n",
    "en_tokens = []\n",
    "\n",
    "for pt, en in train_examples:\n",
    "  pt = pt.numpy().decode('utf-8')\n",
    "  en = en.numpy().decode('utf-8')\n",
    "\n",
    "  # Encode the sentences\n",
    "  pt_encoded = tokenizer.encode(pt)\n",
    "  en_encoded = tokenizer.encode(en)\n",
    "\n",
    "  # Get the tokens\n",
    "  pt_tokens.append(pt_encoded.ids)\n",
    "  en_tokens.append(en_encoded.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17858b63-dd51-44aa-888b-ccfffeeb9beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' e  pelos vistos  o grande profeta de um caso de violncia  um caso precedente de violncia [[[[[[[[[[[[[[[[[[[[[[[[['"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(pt_tokens[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e4c70f-532f-4298-a1bb-35815452bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_tokens = tf.convert_to_tensor(pt_tokens)\n",
    "en_tokens = tf.convert_to_tensor(en_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6141e78-a285-43ba-b9a1-858815759ffc",
   "metadata": {},
   "source": [
    "## Basic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae53725-c610-40f9-8a2d-1a6e4bc8fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "2024-05-22 12:49:08.623323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-05-22 12:49:08.641054: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp_2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810/810 [==============================] - 24s 28ms/step - loss: 2.7951 - sparse_categorical_accuracy: 0.6165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x36d285750>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "\n",
    "BUFFER_SIZE = len(pt_tokens)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create a tf.data.Dataset object for easier shuffling and batched training\n",
    "dataset = tf.data.Dataset.from_tensor_slices((pt_tokens, en_tokens))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Define your Transformer model here\n",
    "# This is a simplified version and might not contain all the components of a full Transformer model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=64),  # You might want to adjust the input_dim parameter depending on your vocabulary size\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10000)  # You might want to adjust this parameter depending on your target vocabulary size\n",
    "])\n",
    "\n",
    "loss = SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = Adam()\n",
    "accuracy = SparseCategoricalAccuracy()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65ef09b-8a07-41fc-bd7b-587f71273770",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ee793e-1a86-4f29-a646-7ebdd577cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = { 'encoder_input': pt_tokens, 'decoder_input': en_tokens[:, :-1] }\n",
    "outputs = en_tokens[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da52e455-6293-414a-9075-5dcee4c14d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a943211f-6371-4c48-8948-5c72cb264b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "Model: \"model_1\"\n",
      "________________________________________________________________________________________________________________________\n",
      " Layer (type)                       Output Shape                        Param #     Connected to                        \n",
      "========================================================================================================================\n",
      " encoder_input (InputLayer)         [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " token_and_position_embedding (Tok  (None, None, 256)                   7692800     ['encoder_input[0][0]']             \n",
      " enAndPositionEmbedding)                                                                                                \n",
      "                                                                                                                        \n",
      " decoder_input (InputLayer)         [(None, None)]                      0           []                                  \n",
      "                                                                                                                        \n",
      " transformer_encoder (TransformerE  (None, None, 256)                   395776      ['token_and_position_embedding[0][0]\n",
      " ncoder)                                                                            ']                                  \n",
      "                                                                                                                        \n",
      " model (Functional)                 (None, None, 30000)                 16062256    ['decoder_input[0][0]',             \n",
      "                                                                                     'transformer_encoder[0][0]']       \n",
      "                                                                                                                        \n",
      "========================================================================================================================\n",
      "Total params: 24150832 (92.13 MB)\n",
      "Trainable params: 24150832 (92.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from keras_nlp.layers import TokenAndPositionEmbedding, TransformerEncoder\n",
    "from keras_nlp.layers import TransformerDecoder\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "num_heads = 8\n",
    "embed_dim = 256\n",
    "\n",
    "encoder_input = Input(shape=(None,), dtype='int64', name='encoder_input')\n",
    "x = TokenAndPositionEmbedding(vocab_size, sequence_len, embed_dim)(encoder_input)\n",
    "encoder_output = TransformerEncoder(embed_dim, num_heads)(x)\n",
    "encoded_seq_input = Input(shape=(None, embed_dim))\n",
    "\n",
    "decoder_input = Input(shape=(None,), dtype='int64', name='decoder_input')\n",
    "x = TokenAndPositionEmbedding(vocab_size, sequence_len, embed_dim, mask_zero=True)(decoder_input)\n",
    "x = TransformerDecoder(embed_dim, num_heads)(x, encoded_seq_input)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "decoder_output = Dense(vocab_size, activation='softmax')(x)\n",
    "decoder = Model([decoder_input, encoded_seq_input], decoder_output)\n",
    "decoder_output = decoder([decoder_input, encoder_output])\n",
    "\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary(line_length=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14742c2f-de6a-4e9f-b9db-61b0d4f6df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1295/1295 [==============================] - 209s 160ms/step - loss: 5.2516 - accuracy: 0.2451 - val_loss: 4.7348 - val_accuracy: 0.2791\n",
      "Epoch 2/10\n",
      "1295/1295 [==============================] - 200s 155ms/step - loss: 4.5211 - accuracy: 0.2923 - val_loss: 4.4204 - val_accuracy: 0.3107\n",
      "Epoch 3/10\n",
      "1295/1295 [==============================] - 200s 154ms/step - loss: 4.0207 - accuracy: 0.3438 - val_loss: 4.0631 - val_accuracy: 0.3626\n",
      "Epoch 4/10\n",
      "1295/1295 [==============================] - 192s 149ms/step - loss: 3.4903 - accuracy: 0.4067 - val_loss: 3.7396 - val_accuracy: 0.4119\n",
      "Epoch 5/10\n",
      "1295/1295 [==============================] - 198s 153ms/step - loss: 3.0282 - accuracy: 0.4615 - val_loss: 3.5304 - val_accuracy: 0.4418\n",
      "Epoch 6/10\n",
      "1295/1295 [==============================] - 196s 151ms/step - loss: 2.6581 - accuracy: 0.5071 - val_loss: 3.4111 - val_accuracy: 0.4587\n",
      "Epoch 7/10\n",
      "1295/1295 [==============================] - 196s 151ms/step - loss: 2.3542 - accuracy: 0.5454 - val_loss: 3.3184 - val_accuracy: 0.4770\n",
      "Epoch 8/10\n",
      "1295/1295 [==============================] - 195s 150ms/step - loss: 2.1122 - accuracy: 0.5788 - val_loss: 3.3064 - val_accuracy: 0.4860\n",
      "Epoch 9/10\n",
      "1295/1295 [==============================] - 196s 151ms/step - loss: 1.9139 - accuracy: 0.6085 - val_loss: 3.2951 - val_accuracy: 0.4901\n",
      "Epoch 10/10\n",
      "1295/1295 [==============================] - 195s 150ms/step - loss: 1.7436 - accuracy: 0.6345 - val_loss: 3.3240 - val_accuracy: 0.4957\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "hist = model.fit(inputs, outputs, epochs=10, validation_split=0.2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3275b214-92e7-435a-abdd-821af25a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, model, tokenizer, sequence_len):\n",
    "\n",
    "    text_encoded = tokenizer.encode(text)\n",
    "    encoder_input = text_encoded.ids\n",
    "    encoder_input = tf.convert_to_tensor(encoder_input)\n",
    "    encoder_input = tf.reshape(encoder_input, (50, 1))\n",
    "    \n",
    "    decoded_text = '[start]'\n",
    "    for i in range(sequence_len):\n",
    "        decoder_input = tokenizer.encode(decoded_text).ids\n",
    "        decoder_input = tf.convert_to_tensor(decoder_input)\n",
    "        decoder_input = tf.reshape(decoder_input, (50, 1))\n",
    "    \n",
    "        prediction = model([encoder_input, decoder_input])\n",
    "    \n",
    "        idx = np.argmax(prediction[i, 0, :])\n",
    "        token = tokenizer.decode([idx])\n",
    "        decoded_text += token\n",
    "\n",
    "        if token == '[end]':\n",
    "            break\n",
    "\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6415e2e6-0768-4d67-a2f5-2ea040feea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura  tiramos a unica vantagem da impressao  que e a serendipidade \n",
      "[start] and when better we the to  tap the and your one so  that and so  it  and  and so and so  so  and  and so and so  so  and  and so and so  so  and  and\n",
      "-------------\n",
      "mas e se estes fatores fossem ativos \n",
      "[start] but and if these the you devices  know  and  and so and so  so  and  and so and so  so  and  and so and so  so  and  and so and so  so  and  and so\n",
      "-------------\n",
      "mas eles nao tinham a curiosidade de me testar \n",
      "[start] but  nation they so communicate  i with   the  hum and  so and  so and  so and  so and  so and  so and  so and  so and  so and  so and  so and  so\n",
      "-------------\n",
      "e esta rebeldia consciente e a razao pela qual eu  como agnostica  posso ainda ter fe \n",
      "[start] and this equally i is and  it so what   as silk i   i still  a  and so  and so  and so  and so  and so  and so  and so  and so  and so  and\n",
      "-------------\n",
      "   podem usar tudo sobre a mesa no meu corpo  \n",
      "[start]   using all a the black no my body strong can  we and know so                                 \n",
      "-------------\n",
      " eu escrevo muito acerca do   teatro de seguranca    que sao produtos que fazem as pessoas sentiremse seguras mas que  na realidade  nao fazem nada  \n",
      "[start]   a about of the theatre one   what i products  you know people  safe  what  in fact  and you do you know  and so  and so  and so  and so  and so  and so  and\n",
      "-------------\n",
      "colocaramno bem no fundo duma mina de ferro no minnesota  nos ultimos dois dias anunciaram os resultados mais sensiveis ate agora \n",
      "[start]  so well no the  one one white no enz people  in  two a and of these more course delight now                          \n",
      "-------------\n",
      "algumas pessoas tem medo de que nao gostem delas \n",
      "[start]  people it they from   trying                                          \n",
      "-------------\n",
      "nao  o que nos aconteceu  chris  e que o poder  o preco esta fixado fora da margem \n",
      "[start]   is what in   chris  and what  the  the filmmaker this inventor used to the same  and so  and so  and so  and so  and so  and so  and so  and so  and so \n",
      "-------------\n",
      "de volta a minha pergunta  porque e que fiquei \n",
      "[start]  and is my the  because and what i                                        \n",
      "-------------\n",
      " quando descobri que havia uma serie de industrias sem protecao de direitos de autor  pensei    qual sera a logica subjacente  \n",
      "[start] when  what i a   lot black   rights  the  i  what the one of course  and so  and so  and so  and so  and so  and so  and so  and so  and so \n",
      "-------------\n",
      "a segunda ronda de votos tambem nao apurou um vencedor claro \n",
      "[start]  second basically and   so highly the after foreign a correspond of  course and  so and  so and  so and  so and  so and  so and  so and  so and  so and  so and  so and\n",
      "-------------\n",
      "a cirurgia foi um sucesso \n",
      "[start]  nature was a   lot                                           \n",
      "-------------\n",
      "em vez disso  viramonos para uma nova geracao de sensores de video inicialmente criados para uso em oculos de visao noturna\n",
      "[start]  once one  you  to  new compens of sensors than a first  to  in compens of order black course to   the  hum and  so and  so and  so and  so and  so and  so and \n",
      "-------------\n",
      "ola  o meu nome e marcin sou agricultor  tecnologo \n",
      "[start]   is my it and  the i american  kim so                                     \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    text = pt_sentences[i]\n",
    "    decoded_text = translate_text(text, model, tokenizer, sequence_len)\n",
    "    print (text)\n",
    "    print (decoded_text)\n",
    "    print (\"-------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
