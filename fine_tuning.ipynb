{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3804a408-261c-4a01-b7dc-2349128822d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Turn on the light</td>\n",
       "      <td>Turn on the light.</td>\n",
       "      <td>Okay, turning on the light.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Play some music</td>\n",
       "      <td>Play some music.</td>\n",
       "      <td>Okay, playing music.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the weather like?</td>\n",
       "      <td>What is the weather like?</td>\n",
       "      <td>The weather is sunny.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 instruction                     prompt  \\\n",
       "0          Turn on the light         Turn on the light.   \n",
       "1            Play some music           Play some music.   \n",
       "2  What is the weather like?  What is the weather like?   \n",
       "\n",
       "                      response  \n",
       "0  Okay, turning on the light.  \n",
       "1         Okay, playing music.  \n",
       "2        The weather is sunny.  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# Create a dictionary with your data\n",
    "data = {\n",
    "    'instruction': ['Turn on the light', 'Play some music', 'What is the weather like?'],\n",
    "    'prompt': ['Turn on the light.', 'Play some music.', 'What is the weather like?'],\n",
    "    'response': ['Okay, turning on the light.', 'Okay, playing music.', 'The weather is sunny.']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b290241-1056-45e6-8005-904c2be5a2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def data_preparation(df):\n",
    "\n",
    "    # Convert to dictionary \n",
    "    data_dict = df.to_dict('list')\n",
    "\n",
    "    # To Huggingface Dataset\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def data_tokenization(dataset):\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "    tokenizer.add_eos_token = True\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    def tokenize(text):\n",
    "        result = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        # \"self-supervised learning\" means the labels are also the inputs:\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def tokenize_chatbot_text(data_point):\n",
    "        full_text =f\"\"\"You are an AI assistant. Your job is to create a correct json format based on the questions.\n",
    "\n",
    "                        If you do not know, please say \"I am sorry, please try again.\".\n",
    "\n",
    "                    ### Instruction:\n",
    "                    {data_point[\"instruction\"]}\n",
    "\n",
    "                    ### Prompt:\n",
    "                    {data_point[\"prompt\"]}\n",
    "\n",
    "                    ### Response:\n",
    "                    {data_point[\"response\"]}\n",
    "                    \"\"\"\n",
    "        return tokenize(full_text)\n",
    "    \n",
    "    tokenized_dataset = dataset.map(tokenize_chatbot_text)\n",
    "    \n",
    "    return tokenizer, tokenized_dataset\n",
    "\n",
    "\n",
    "def model_preparation():\n",
    "    base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    model.train() # put model back into training mode\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    resume_from_checkpoint = \"\" # set this to the adapter_model.bin file you want to resume from\n",
    "\n",
    "    if resume_from_checkpoint:\n",
    "        if os.path.exists(resume_from_checkpoint):\n",
    "            print(f\"Restarting from {resume_from_checkpoint}\")\n",
    "            adapters_weights = torch.load(resume_from_checkpoint)\n",
    "            set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {resume_from_checkpoint} not found\")\n",
    "\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def model_training(model, tokenizer, tokenized_dataset_train, tokenized_dataset_val):\n",
    "\n",
    "    batch_size = 4\n",
    "    per_device_train_batch_size = 2\n",
    "    gradient_accumulation_steps = batch_size // per_device_train_batch_size\n",
    "    output_dir = \"checkpoints\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            max_steps=400,\n",
    "            learning_rate=3e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=20,\n",
    "            save_steps=20,\n",
    "            output_dir=output_dir,\n",
    "            load_best_model_at_end=False,\n",
    "            group_by_length=True, # group sequences of roughly the same length together to speed up training\n",
    "            run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n",
    "        )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_dataset_train,\n",
    "        eval_dataset=tokenized_dataset_val,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n",
    "        model, type(model)\n",
    "    )\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        print(\"compiling the model\")\n",
    "        model = torch.compile(model)\n",
    "        \n",
    "    # Train now\n",
    "    trainer.train()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29897b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = data_preparation(df)\n",
    "\n",
    "tokenizer, tokenized_dataset_train = data_tokenization(dataset_train)\n",
    "\n",
    "model = model_preparation()\n",
    "\n",
    "model_training(model, tokenizer, tokenized_dataset_train, tokenized_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5f351c-591f-46da-8873-a63f9541ab5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844ff1ffb7624de180652ebff6fdf00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = \"codellama/CodeLlama-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4583307-b413-4eeb-bad3-8358a06d224f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = 'checkpoints/checkpoint-100'\n",
    "model = PeftModel.from_pretrained(model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d21eae8-38ef-437b-94f9-f2d664f69561",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant. Your job is to create a correct json format based on the questions.\n",
      "\n",
      "                If you do not know, please say \"I am sorry, please try again.\n",
      "                ### Instruction:\n",
      "                This is Instruction.\n",
      "\n",
      "                ### Prompt:\n",
      "                How are you?\n",
      "\n",
      "                ### Response:\n",
      "                 You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant.\n",
      "                 You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant.\n",
      "                 You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant.\n",
      "                 You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant.\n",
      "                 You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant. You are an AI assistant.\n",
      "                 You are an AI assistant. You are an A\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"You are an AI assistant. Your job is to create a correct json format based on the questions.\n",
    "\n",
    "                If you do not know, please say \"I am sorry, please try again.\n",
    "                ### Instruction:\n",
    "                This is Instruction.\n",
    "\n",
    "                ### Prompt:\n",
    "                How are you?\n",
    "\n",
    "                ### Response:\n",
    "                \"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=200)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
