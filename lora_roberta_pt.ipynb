{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3332f4-47a8-4a2c-8968-c2aa60dec1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a7e79e-f2e2-496f-a624-2e65945bb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanvir.islam/miniconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with LoRA param count: 595968\n",
      "Base model param count: 125237762\n",
      "210 times smaller than base model\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial\n",
    "import transformers\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"true\"\n",
    "\n",
    "class RobertaBaseClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaBaseClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.linear = torch.nn.Linear(768, 768)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_with_pooling = self.roberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        hidden_state = output_with_pooling[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.linear(pooler)\n",
    "        pooler = self.activation(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, r, alpha):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Initialize A to kaiming uniform following code: https://github.com/microsoft/LoRA/blob/main/loralib/layers.py\n",
    "        self.A = torch.nn.Parameter(torch.empty(r, in_dim))\n",
    "        # Initialize B to zeros.\n",
    "        self.B = torch.nn.Parameter(torch.empty(out_dim, r))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        torch.nn.init.zeros_(self.B)\n",
    "\n",
    "        self.scaling = self.alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.scaling * (x @ self.A.transpose(0, 1) @ self.B.transpose(0, 1))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, r, alpha):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Initialize A, B, and C matrices\n",
    "        self.A = torch.nn.Parameter(torch.empty(r, in_dim))\n",
    "        self.B = torch.nn.Parameter(torch.empty(out_dim, r))\n",
    "        self.C = torch.nn.Parameter(torch.empty(r, r))\n",
    "\n",
    "        # Initialize A and C with Kaiming uniform\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        torch.nn.init.kaiming_uniform_(self.C, a=math.sqrt(5))\n",
    "\n",
    "        # Initialize B to zeros to start with a \"no adaptation\" setup\n",
    "        torch.nn.init.zeros_(self.B)\n",
    "\n",
    "        # Scaling factor for LoRA\n",
    "        self.scaling = self.alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform tri-factorized transformation: x * A * C * B\n",
    "        # Optional nonlinearity after A or C (e.g., ReLU), can be adjusted\n",
    "        x = x @ self.A.T                    # Step 1: x * A.T\n",
    "        x = F.relu(x @ self.C)               # Step 2: Apply C and optional ReLU for nonlinearity\n",
    "        x = x @ self.B.T                     # Step 3: Apply B.T\n",
    "\n",
    "        # Apply scaling\n",
    "        x = self.scaling * x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, r, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, r, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "class LoraModel(nn.Module):\n",
    "    def __init__(self, base_model_name=\"roberta-base\", rank=16, dropout_rate=0.1):\n",
    "        super(LoraModel, self).__init__()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained(base_model_name)\n",
    "\n",
    "        # Freeze RoBERTa parameters\n",
    "        for param in self.roberta.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Initialize LoRA parameters\n",
    "        self.rank = rank\n",
    "        self.lora_A = nn.Linear(\n",
    "            self.roberta.config.hidden_size, rank, bias=False\n",
    "        )  # Size: 768 x 16\n",
    "        self.lora_B = nn.Linear(\n",
    "            rank, self.roberta.config.hidden_size, bias=False\n",
    "        )  # Size: 16 x 768\n",
    "\n",
    "        # Output layer for binary classification\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size, 2)  # Size: 768 x 2\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        roberta_output = self.roberta(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "        hidden_states = roberta_output.last_hidden_state\n",
    "\n",
    "        # Apply LoRA\n",
    "        lora_output = self.lora_B(self.lora_A(hidden_states))\n",
    "\n",
    "        output = hidden_states + lora_output\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        cls_output = output[:, 0, :]  # Use the [CLS] token representation\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def create_lora_model():\n",
    "\n",
    "    lora_model = RobertaBaseClassifier()\n",
    "\n",
    "    for param in lora_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    lora_r = 16\n",
    "    lora_alpha = lora_r * 2\n",
    "    \n",
    "    assign_lora = partial(LinearWithLoRA, r=lora_r, alpha=lora_alpha)\n",
    "    \n",
    "    for layer in lora_model.roberta.encoder.layer:\n",
    "        layer.attention.self.query = assign_lora(layer.attention.self.query)\n",
    "        layer.attention.self.value = assign_lora(layer.attention.self.value)\n",
    "\n",
    "    return lora_model\n",
    "\n",
    "roberta_model = RobertaBaseClassifier()\n",
    "base_param_count = count_parameters(roberta_model)\n",
    "\n",
    "lora_model = create_lora_model()\n",
    "lora_param_count = count_parameters(lora_model)\n",
    "print(\"Model with LoRA param count:\", lora_param_count)\n",
    "print(\"Base model param count:\", base_param_count)\n",
    "print(str(base_param_count // lora_param_count) + \" times smaller than base model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5679f0e-cd8e-4bb8-b13a-d795b9abfe0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RobertaBaseClassifier                                             --\n",
       "├─RobertaModel: 1-1                                               --\n",
       "│    └─RobertaEmbeddings: 2-1                                     --\n",
       "│    │    └─Embedding: 3-1                                        38,603,520\n",
       "│    │    └─Embedding: 3-2                                        394,752\n",
       "│    │    └─Embedding: 3-3                                        768\n",
       "│    │    └─LayerNorm: 3-4                                        1,536\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─RobertaEncoder: 2-2                                        --\n",
       "│    │    └─ModuleList: 3-6                                       85,054,464\n",
       "│    └─RobertaPooler: 2-3                                         --\n",
       "│    │    └─Linear: 3-7                                           590,592\n",
       "│    │    └─Tanh: 3-8                                             --\n",
       "├─Linear: 1-2                                                     590,592\n",
       "├─ReLU: 1-3                                                       --\n",
       "├─Dropout: 1-4                                                    --\n",
       "├─Linear: 1-5                                                     1,538\n",
       "==========================================================================================\n",
       "Total params: 125,237,762\n",
       "Trainable params: 125,237,762\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(roberta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c34ee97-bc22-4f38-8242-0068ff364193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RobertaBaseClassifier                                             --\n",
       "├─RobertaModel: 1-1                                               --\n",
       "│    └─RobertaEmbeddings: 2-1                                     --\n",
       "│    │    └─Embedding: 3-1                                        (38,603,520)\n",
       "│    │    └─Embedding: 3-2                                        (394,752)\n",
       "│    │    └─Embedding: 3-3                                        (768)\n",
       "│    │    └─LayerNorm: 3-4                                        (1,536)\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─RobertaEncoder: 2-2                                        --\n",
       "│    │    └─ModuleList: 3-6                                       85,650,432\n",
       "│    └─RobertaPooler: 2-3                                         --\n",
       "│    │    └─Linear: 3-7                                           (590,592)\n",
       "│    │    └─Tanh: 3-8                                             --\n",
       "├─Linear: 1-2                                                     (590,592)\n",
       "├─ReLU: 1-3                                                       --\n",
       "├─Dropout: 1-4                                                    --\n",
       "├─Linear: 1-5                                                     (1,538)\n",
       "==========================================================================================\n",
       "Total params: 125,833,730\n",
       "Trainable params: 595,968\n",
       "Non-trainable params: 125,237,762\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c3ed08-ed6b-441a-bb2c-ac04fc8f0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, param in roberta_model.named_parameters():\n",
    "#    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf49228e-4890-4fee-a624-0dd7fb20f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], return_token_type_ids=True, truncation=True)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", truncation=True, do_lower_case=True)\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e15f437-61f4-40d9-9fcc-0864fcf37635",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a09eea46-5343-4763-9be3-d63eb8fcf776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 87]),\n",
       " 'token_type_ids': torch.Size([8, 87]),\n",
       " 'attention_mask': torch.Size([8, 87])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a89b55-0258-4c99-864b-aed3bba8c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff7e7332-fe64-4b32-b30c-0e3148e84d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c642aca8-ea8f-4d03-9468-f788e335910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "labels = batch[\"labels\"].to(device)\n",
    "model.to(device)\n",
    "\n",
    "outputs = model(input_ids,\n",
    "              attention_mask=attention_mask,\n",
    "              token_type_ids=token_type_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341aef91-60aa-48d7-9817-a4bdcbb74415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 91/459 | Training loss: 0.6170 | accuracy: 0.8750\n",
      "Batch: 182/459 | Training loss: 0.7338 | accuracy: 0.5000\n",
      "Batch: 273/459 | Training loss: 0.7314 | accuracy: 0.5000\n",
      "Batch: 364/459 | Training loss: 0.6421 | accuracy: 0.7500\n",
      "Batch: 455/459 | Training loss: 0.6508 | accuracy: 0.6250\n",
      "Epoch: 1 | Train Loss: 0.6519 | Train Acc: 0.6743\n",
      "Epoch time elapsed: 00:00:44.14\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.6932 | accuracy: 0.5000\n",
      "Batch: 182/459 | Training loss: 0.6254 | accuracy: 0.6250\n",
      "Batch: 273/459 | Training loss: 0.6159 | accuracy: 0.6250\n",
      "Batch: 364/459 | Training loss: 0.5178 | accuracy: 0.8750\n",
      "Batch: 455/459 | Training loss: 0.7187 | accuracy: 0.5000\n",
      "Epoch: 2 | Train Loss: 0.6413 | Train Acc: 0.6743\n",
      "Epoch time elapsed: 00:00:36.75\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.5279 | accuracy: 0.8750\n",
      "Batch: 182/459 | Training loss: 0.3549 | accuracy: 1.0000\n",
      "Batch: 273/459 | Training loss: 0.4660 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.5730 | accuracy: 0.6250\n",
      "Batch: 455/459 | Training loss: 0.6212 | accuracy: 0.6250\n",
      "Epoch: 3 | Train Loss: 0.6058 | Train Acc: 0.6746\n",
      "Epoch time elapsed: 00:00:37.12\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.4867 | accuracy: 0.7500\n",
      "Batch: 182/459 | Training loss: 0.8538 | accuracy: 0.2500\n",
      "Batch: 273/459 | Training loss: 0.4581 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.8673 | accuracy: 0.5000\n",
      "Batch: 455/459 | Training loss: 0.6369 | accuracy: 0.6250\n",
      "Epoch: 4 | Train Loss: 0.5838 | Train Acc: 0.6746\n",
      "Epoch time elapsed: 00:00:36.20\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.7019 | accuracy: 0.6250\n",
      "Batch: 182/459 | Training loss: 0.5692 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.4798 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.7439 | accuracy: 0.5000\n",
      "Batch: 455/459 | Training loss: 0.6566 | accuracy: 0.6250\n",
      "Epoch: 5 | Train Loss: 0.5728 | Train Acc: 0.6746\n",
      "Epoch time elapsed: 00:00:37.60\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.4443 | accuracy: 0.6250\n",
      "Batch: 182/459 | Training loss: 0.5352 | accuracy: 0.6250\n",
      "Batch: 273/459 | Training loss: 0.3412 | accuracy: 0.8750\n",
      "Batch: 364/459 | Training loss: 0.5140 | accuracy: 0.6250\n",
      "Batch: 455/459 | Training loss: 0.2899 | accuracy: 0.8750\n",
      "Epoch: 6 | Train Loss: 0.5531 | Train Acc: 0.6748\n",
      "Epoch time elapsed: 00:00:36.39\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.6373 | accuracy: 0.6250\n",
      "Batch: 182/459 | Training loss: 0.4165 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.5678 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.3912 | accuracy: 0.7500\n",
      "Batch: 455/459 | Training loss: 0.5590 | accuracy: 0.7500\n",
      "Epoch: 7 | Train Loss: 0.5321 | Train Acc: 0.6740\n",
      "Epoch time elapsed: 00:00:36.97\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.6651 | accuracy: 0.6250\n",
      "Batch: 182/459 | Training loss: 0.3209 | accuracy: 0.8750\n",
      "Batch: 273/459 | Training loss: 0.4212 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.3647 | accuracy: 1.0000\n",
      "Batch: 455/459 | Training loss: 0.6173 | accuracy: 0.6250\n",
      "Epoch: 8 | Train Loss: 0.5161 | Train Acc: 0.6746\n",
      "Epoch time elapsed: 00:00:37.26\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.7453 | accuracy: 0.2500\n",
      "Batch: 182/459 | Training loss: 0.8537 | accuracy: 0.3750\n",
      "Batch: 273/459 | Training loss: 0.3367 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.3025 | accuracy: 0.8750\n",
      "Batch: 455/459 | Training loss: 0.6156 | accuracy: 0.6250\n",
      "Epoch: 9 | Train Loss: 0.4923 | Train Acc: 0.6754\n",
      "Epoch time elapsed: 00:00:37.99\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.4705 | accuracy: 0.7500\n",
      "Batch: 182/459 | Training loss: 0.5533 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.4289 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.4295 | accuracy: 0.8750\n",
      "Batch: 455/459 | Training loss: 0.3263 | accuracy: 0.8750\n",
      "Epoch: 10 | Train Loss: 0.4667 | Train Acc: 0.6833\n",
      "Epoch time elapsed: 00:00:38.28\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.3098 | accuracy: 0.7500\n",
      "Batch: 182/459 | Training loss: 0.4856 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.6175 | accuracy: 0.3750\n",
      "Batch: 364/459 | Training loss: 0.5797 | accuracy: 0.7500\n",
      "Batch: 455/459 | Training loss: 0.5614 | accuracy: 0.6250\n",
      "Epoch: 11 | Train Loss: 0.4536 | Train Acc: 0.7198\n",
      "Epoch time elapsed: 00:00:37.00\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.6139 | accuracy: 0.6250\n",
      "Batch: 182/459 | Training loss: 0.4031 | accuracy: 0.6250\n",
      "Batch: 273/459 | Training loss: 0.3593 | accuracy: 0.8750\n",
      "Batch: 364/459 | Training loss: 0.4023 | accuracy: 0.6250\n",
      "Batch: 455/459 | Training loss: 0.3511 | accuracy: 0.8750\n",
      "Epoch: 12 | Train Loss: 0.4266 | Train Acc: 0.7963\n",
      "Epoch time elapsed: 00:00:39.74\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.2069 | accuracy: 1.0000\n",
      "Batch: 182/459 | Training loss: 0.4190 | accuracy: 0.8750\n",
      "Batch: 273/459 | Training loss: 0.2528 | accuracy: 0.8750\n",
      "Batch: 364/459 | Training loss: 0.4173 | accuracy: 0.5000\n",
      "Batch: 455/459 | Training loss: 0.1805 | accuracy: 1.0000\n",
      "Epoch: 13 | Train Loss: 0.4132 | Train Acc: 0.8113\n",
      "Epoch time elapsed: 00:00:41.66\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.9013 | accuracy: 0.6250\n",
      "Batch: 182/459 | Training loss: 0.3917 | accuracy: 0.8750\n",
      "Batch: 273/459 | Training loss: 0.3747 | accuracy: 0.8750\n",
      "Batch: 364/459 | Training loss: 0.3961 | accuracy: 0.8750\n",
      "Batch: 455/459 | Training loss: 0.5011 | accuracy: 0.5000\n",
      "Epoch: 14 | Train Loss: 0.4007 | Train Acc: 0.8186\n",
      "Epoch time elapsed: 00:00:39.60\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.4467 | accuracy: 0.7500\n",
      "Batch: 182/459 | Training loss: 0.3372 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 1.0126 | accuracy: 0.6250\n",
      "Batch: 364/459 | Training loss: 0.1856 | accuracy: 1.0000\n",
      "Batch: 455/459 | Training loss: 0.4891 | accuracy: 0.7500\n",
      "Epoch: 15 | Train Loss: 0.3971 | Train Acc: 0.8219\n",
      "Epoch time elapsed: 00:00:39.83\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.3613 | accuracy: 0.7500\n",
      "Batch: 182/459 | Training loss: 0.3229 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.3693 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.5959 | accuracy: 0.7500\n",
      "Batch: 455/459 | Training loss: 0.5516 | accuracy: 0.8750\n",
      "Epoch: 16 | Train Loss: 0.3817 | Train Acc: 0.8282\n",
      "Epoch time elapsed: 00:00:40.07\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.2550 | accuracy: 0.8750\n",
      "Batch: 182/459 | Training loss: 0.1950 | accuracy: 0.8750\n",
      "Batch: 273/459 | Training loss: 0.9233 | accuracy: 0.7500\n",
      "Batch: 364/459 | Training loss: 0.3375 | accuracy: 0.7500\n",
      "Batch: 455/459 | Training loss: 0.2149 | accuracy: 1.0000\n",
      "Epoch: 17 | Train Loss: 0.3730 | Train Acc: 0.8363\n",
      "Epoch time elapsed: 00:00:40.50\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.1904 | accuracy: 0.8750\n",
      "Batch: 182/459 | Training loss: 0.1741 | accuracy: 1.0000\n",
      "Batch: 273/459 | Training loss: 0.4209 | accuracy: 0.8750\n",
      "Batch: 364/459 | Training loss: 0.2510 | accuracy: 0.8750\n",
      "Batch: 455/459 | Training loss: 0.2801 | accuracy: 0.8750\n",
      "Epoch: 18 | Train Loss: 0.3681 | Train Acc: 0.8420\n",
      "Epoch time elapsed: 00:00:39.87\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.3156 | accuracy: 0.8750\n",
      "Batch: 182/459 | Training loss: 0.4700 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.2703 | accuracy: 1.0000\n",
      "Batch: 364/459 | Training loss: 0.2978 | accuracy: 0.8750\n",
      "Batch: 455/459 | Training loss: 0.4092 | accuracy: 0.7500\n",
      "Epoch: 19 | Train Loss: 0.3580 | Train Acc: 0.8434\n",
      "Epoch time elapsed: 00:00:39.96\n",
      "\n",
      "Batch: 91/459 | Training loss: 0.2854 | accuracy: 0.7500\n",
      "Batch: 182/459 | Training loss: 0.4797 | accuracy: 0.7500\n",
      "Batch: 273/459 | Training loss: 0.1599 | accuracy: 1.0000\n",
      "Batch: 364/459 | Training loss: 0.1898 | accuracy: 1.0000\n",
      "Batch: 455/459 | Training loss: 0.2478 | accuracy: 1.0000\n",
      "Epoch: 20 | Train Loss: 0.3585 | Train Acc: 0.8388\n",
      "Epoch time elapsed: 00:00:41.62\n",
      "\n",
      "Average time per epoch: 00:00:38.93\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_accuracy(y_pred, targets):\n",
    "    predictions = torch.log_softmax(y_pred, dim=1).argmax(dim=1)\n",
    "    accuracy = (predictions == targets).sum() / len(targets)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs, optimizer):\n",
    "    total_time = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        interval = len(train_loader) // 5\n",
    "\n",
    "        total_train_loss = 0\n",
    "        total_train_acc = 0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            loss = loss_function(outputs, labels)\n",
    "            acc = get_accuracy(outputs, labels)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_acc += acc.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch_idx + 1) % interval == 0:\n",
    "                print(\n",
    "                    \"Batch: %s/%s | Training loss: %.4f | accuracy: %.4f\"\n",
    "                    % (batch_idx + 1, len(train_loader), loss, acc)\n",
    "                )\n",
    "\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        train_acc = total_train_acc / len(train_loader)\n",
    "\n",
    "        end = time.time()\n",
    "        hours, remainder = divmod(end - start, 3600)\n",
    "        minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        #print(f\"Epoch: {epoch+1} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(\n",
    "            \"Epoch time elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(\n",
    "                int(hours), int(minutes), seconds\n",
    "            )\n",
    "        )\n",
    "        print(\"\")\n",
    "\n",
    "        total_time += end - start\n",
    "\n",
    "    # Get the average time per epoch\n",
    "    average_time_per_epoch = total_time / epochs\n",
    "    hours, remainder = divmod(average_time_per_epoch, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    print(\n",
    "        \"Average time per epoch: {:0>2}:{:0>2}:{:05.2f}\".format(\n",
    "            int(hours), int(minutes), seconds\n",
    "        )\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 1e-5)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "train(model, train_dataloader, val_dataloader, 20, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a2d482b-5145-4399-874b-dd7a9bdf792e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RobertaBaseClassifier                                             --\n",
       "├─RobertaModel: 1-1                                               --\n",
       "│    └─RobertaEmbeddings: 2-1                                     --\n",
       "│    │    └─Embedding: 3-1                                        (38,603,520)\n",
       "│    │    └─Embedding: 3-2                                        (394,752)\n",
       "│    │    └─Embedding: 3-3                                        (768)\n",
       "│    │    └─LayerNorm: 3-4                                        (1,536)\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─RobertaEncoder: 2-2                                        --\n",
       "│    │    └─ModuleList: 3-6                                       85,650,432\n",
       "│    └─RobertaPooler: 2-3                                         --\n",
       "│    │    └─Linear: 3-7                                           (590,592)\n",
       "│    │    └─Tanh: 3-8                                             --\n",
       "├─Linear: 1-2                                                     (590,592)\n",
       "├─ReLU: 1-3                                                       --\n",
       "├─Dropout: 1-4                                                    --\n",
       "├─Linear: 1-5                                                     (1,538)\n",
       "==========================================================================================\n",
       "Total params: 125,833,730\n",
       "Trainable params: 595,968\n",
       "Non-trainable params: 125,237,762\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebc6ee8-fed0-41e5-a949-ebf34e630a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 10/51 | Test loss: 0.4802 | accuracy: 0.8750\n",
      "Batch: 20/51 | Test loss: 0.3231 | accuracy: 0.7500\n",
      "Batch: 30/51 | Test loss: 0.6541 | accuracy: 0.5000\n",
      "Batch: 40/51 | Test loss: 0.5715 | accuracy: 0.7500\n",
      "Batch: 50/51 | Test loss: 0.1695 | accuracy: 0.8750\n",
      "Test loss: 0.3642 acc: 0.8358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    interval = len(test_loader) // 5\n",
    "\n",
    "    total_test_loss = 0\n",
    "    total_test_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = loss_function(outputs, labels)\n",
    "            acc = get_accuracy(outputs, labels)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_acc += acc.item()\n",
    "\n",
    "            if (batch_idx + 1) % interval == 0:\n",
    "                print(\n",
    "                    \"Batch: %s/%s | Test loss: %.4f | accuracy: %.4f\"\n",
    "                    % (batch_idx + 1, len(test_loader), loss, acc)\n",
    "                )\n",
    "\n",
    "    test_loss = total_test_loss / len(test_loader)\n",
    "    test_acc = total_test_acc / len(test_loader)\n",
    "\n",
    "    print(f\"Test loss: {test_loss:.4f} acc: {test_acc:.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "evaluate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dfe61d9-d1d8-458d-817c-23e29fa2e7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "RobertaBaseClassifier                                             --\n",
       "├─RobertaModel: 1-1                                               --\n",
       "│    └─RobertaEmbeddings: 2-1                                     --\n",
       "│    │    └─Embedding: 3-1                                        (38,603,520)\n",
       "│    │    └─Embedding: 3-2                                        (394,752)\n",
       "│    │    └─Embedding: 3-3                                        (768)\n",
       "│    │    └─LayerNorm: 3-4                                        (1,536)\n",
       "│    │    └─Dropout: 3-5                                          --\n",
       "│    └─RobertaEncoder: 2-2                                        --\n",
       "│    │    └─ModuleList: 3-6                                       85,650,432\n",
       "│    └─RobertaPooler: 2-3                                         --\n",
       "│    │    └─Linear: 3-7                                           (590,592)\n",
       "│    │    └─Tanh: 3-8                                             --\n",
       "├─Linear: 1-2                                                     (590,592)\n",
       "├─ReLU: 1-3                                                       --\n",
       "├─Dropout: 1-4                                                    --\n",
       "├─Linear: 1-5                                                     (1,538)\n",
       "==========================================================================================\n",
       "Total params: 125,833,730\n",
       "Trainable params: 595,968\n",
       "Non-trainable params: 125,237,762\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be2c9e-be0e-44c0-8e0e-1261bd9357cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
